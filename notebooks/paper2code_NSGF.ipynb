{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-cache-dir \"litellm==1.77.4\" \"openai==1.99.9\"  \"dspy-ai==3.0.3\"\n",
    "%pip install 'litellm[proxy]' --quiet\n",
    "%pip install requests --quiet\n",
    "%pip install -q  wandb==0.22.0 \n",
    "%pip install -q weave\n",
    "%pip install -q groq\n",
    "%pip install -q cerebras_cloud_sdk\n",
    "%pip install pyyaml --quiet\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import logging\n",
    "import yaml\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logging.info(\"Logging is now visible!\")\n",
    "\n",
    "# Load secrets\n",
    "if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    os.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "    os.environ[\"GROQ_API_KEY_153\"] = user_secrets.get_secret(\"GROQ_API_KEY_153\")\n",
    "    os.environ[\"CEREBRAS_API_KEY\"] = user_secrets.get_secret(\"CEREBRAS_API_KEY\")\n",
    "    os.environ[\"CEREBRAS_API_KEY_153\"] = user_secrets.get_secret(\"CEREBRAS_API_KEY_153\")\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = user_secrets.get_secret('OPENROUTER_API_KEY')\n",
    "    os.environ[\"OPENROUTER_API_KEY_153\"] = user_secrets.get_secret('OPENROUTER_API_KEY_153')\n",
    "else:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
    "    os.environ[\"GROQ_API_KEY_153\"] = userdata.get('GROQ_API_KEY_153')\n",
    "    os.environ[\"CEREBRAS_API_KEY\"] = userdata.get('CEREBRAS_API_KEY')\n",
    "    os.environ[\"CEREBRAS_API_KEY_153\"] = userdata.get('CEREBRAS_API_KEY_153')\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = userdata.get('OPENROUTER_API_KEY')\n",
    "    os.environ[\"OPENROUTER_API_KEY_153\"] = userdata.get('OPENROUTER_API_KEY_153')\n",
    "\n",
    "# Constants for environment names\n",
    "KAGGLE = \"Kaggle\"\n",
    "COLAB = \"Colab\"\n",
    "LOCAL = \"Local\"\n",
    "\n",
    "def get_python_major_minor():\n",
    "    version_tuple = sys.version_info[:2]\n",
    "    return f\"{version_tuple[0]}.{version_tuple[1]}\"\n",
    "\n",
    "def get_environment():\n",
    "    \"\"\"\n",
    "    Determines the current execution environment and retrieves relevant settings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - environment (str): The name of the detected environment.\n",
    "            - working_dir (str): The working directory path.\n",
    "            - python_version (str): The version of Python in use.\n",
    "            - wandb_api_key (str or None): The Wandb API key, if available.\n",
    "    \"\"\"\n",
    "    environment = LOCAL\n",
    "    working_dir = os.getcwd()\n",
    "    python_version = sys.version.split()[0]\n",
    "    wandb_api_key = None\n",
    "\n",
    "    python_version = get_python_major_minor()\n",
    "\n",
    "    if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            user_secrets = UserSecretsClient()\n",
    "            wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "            environment = KAGGLE\n",
    "            working_dir = \"/kaggle/working/\"\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"Error: Unable to import Kaggle secrets module.\")\n",
    "    elif \"google.colab\" in sys.modules:\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            environment = COLAB\n",
    "            working_dir = \"/content\"\n",
    "\n",
    "            wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "            if not wandb_api_key:\n",
    "                print(\"Warning: WANDB_API_KEY not found in Colab userdata.\")\n",
    "        except ImportError:\n",
    "            print(\"Error: Unable to import Colab userdata module.\")\n",
    "    else:\n",
    "        print(\"Local environment detected.\")\n",
    "\n",
    "    return (environment, working_dir, python_version, wandb_api_key,)\n",
    "\n",
    "def display_status(message, status=\"info\"):\n",
    "    if status == \"error\":\n",
    "        logging.error(f\"[{status.upper()}] {message}\")\n",
    "    elif status == \"warning\":\n",
    "        logging.warning(f\"[{status.upper()}] {message}\")\n",
    "    else:\n",
    "        logging.info(f\"[{status.upper()}] {message}\")\n",
    "\n",
    "def setup_and_start_ollama(models_to_pull=None):\n",
    "    if models_to_pull is None:\n",
    "        models_to_pull = [\n",
    "            \"hf.co/Qwen/Qwen3-8B-GGUF:Q8_0\"\n",
    "        ]\n",
    "\n",
    "    print(\"Installing Ollama... This may take a minute...\")\n",
    "    result = os.system(\"curl -fsSL https://ollama.com/install.sh | sh 2>/dev/null\")\n",
    "    if result == 0:\n",
    "        display_status(\"âœ… Ollama installed successfully!\", \"success\")\n",
    "    else:\n",
    "        display_status(\"âš ï¸ Ollama installation had warnings but may still work\", \"warning\")\n",
    "\n",
    "    print(\"Starting Ollama server...\")\n",
    "    os.system(\"nohup ollama serve > /tmp/ollama_serve_stdout.log 2>/tmp/ollama_serve_stderr.log &\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    running = os.system(\"ps aux | grep -E 'ollama serve' | grep -v grep > /dev/null 2>&1\")\n",
    "    if running == 0:\n",
    "        display_status(\"âœ… Ollama server is running!\", \"success\")\n",
    "    else:\n",
    "        display_status(\"âŒ Ollama server failed to start. Check troubleshooting section.\", \"error\")\n",
    "\n",
    "    for model in models_to_pull:\n",
    "        display_status(f\"ðŸ“¦ Pulling model: {model}\")\n",
    "        start_time = time.time()\n",
    "        result = os.system(f\"ollama pull {model}\")\n",
    "        end_time = time.time()\n",
    "\n",
    "        if result == 0:\n",
    "            elapsed = end_time - start_time\n",
    "            display_status(f\"âœ… Model '{model}' downloaded in {elapsed/60:.1f} minutes!\", \"success\")\n",
    "        else:\n",
    "            display_status(f\"âŒ Failed to download model '{model}'.\", \"error\")\n",
    "\n",
    "    print(\"\\nðŸ“‹ Available models:\")\n",
    "    time.sleep(30)\n",
    "    os.system(\"ollama list\")\n",
    "\n",
    "def create_litellm_config(config_path, environment):\n",
    "    \"\"\"\n",
    "    Creates LiteLLM proxy config with your exact models and multiple accounts.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to save the config file\n",
    "        environment: Current environment (KAGGLE, COLAB, or LOCAL)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base config with cloud providers\n",
    "    config_content = \"\"\"\n",
    "model_list:\n",
    "  # OpenRouter models\n",
    "  - model_name: deepseek-chat-v3\n",
    "    litellm_params:\n",
    "      model: openrouter/deepseek/deepseek-chat-v3.1:free\n",
    "      api_key: os.environ/OPENROUTER_API_KEY\n",
    "  - model_name: deepseek-chat-v3\n",
    "    litellm_params:\n",
    "      model: openrouter/deepseek/deepseek-chat-v3.1:free\n",
    "      api_key: os.environ/OPENROUTER_API_KEY_153\n",
    "      \n",
    "  # Cerebras Account 1\n",
    "  - model_name: o3-mini\n",
    "    litellm_params:\n",
    "      model: cerebras/gpt-oss-120b\n",
    "      api_key: os.environ/CEREBRAS_API_KEY\n",
    "  - model_name: qwen-3-235b-a22b-thinking-2507\n",
    "    litellm_params:\n",
    "      model: cerebras/qwen-3-235b-a22b-thinking-2507\n",
    "      api_key: os.environ/CEREBRAS_API_KEY\n",
    "  - model_name: qwen-3-235b-a22b-instruct-2507\n",
    "    litellm_params:\n",
    "      model: cerebras/qwen-3-235b-a22b-instruct-2507\n",
    "      api_key: os.environ/CEREBRAS_API_KEY\n",
    "      \n",
    "  # Cerebras Account 2 (153)\n",
    "  - model_name: o3-mini_153\n",
    "    litellm_params:\n",
    "      model: cerebras/gpt-oss-120b\n",
    "      api_key: os.environ/CEREBRAS_API_KEY_153\n",
    "  - model_name: qwen-3-235b-a22b-thinking-2507_153\n",
    "    litellm_params:\n",
    "      model: cerebras/qwen-3-235b-a22b-thinking-2507\n",
    "      api_key: os.environ/CEREBRAS_API_KEY_153\n",
    "  - model_name: qwen-3-235b-a22b-instruct-2507_153\n",
    "    litellm_params:\n",
    "      model: cerebras/qwen-3-235b-a22b-instruct-2507\n",
    "      api_key: os.environ/CEREBRAS_API_KEY_153\n",
    "      \n",
    "  # Groq models\n",
    "  - model_name: gpt-oss-120b\n",
    "    litellm_params:\n",
    "      model: groq/openai/gpt-oss-120b\n",
    "      api_key: os.environ/GROQ_API_KEY\n",
    "      api_base: https://api.groq.com/openai/v1\n",
    "  - model_name: gpt-oss-120b\n",
    "    litellm_params:\n",
    "      model: groq/openai/gpt-oss-120b\n",
    "      api_key: os.environ/GROQ_API_KEY_153\n",
    "      api_base: https://api.groq.com/openai/v1\n",
    "  - model_name: llama-3.3-70b-versatile\n",
    "    litellm_params:\n",
    "      model: groq/llama-3.3-70b-versatile\n",
    "      api_key: os.environ/GROQ_API_KEY\n",
    "      api_base: https://api.groq.com/openai/v1\n",
    "  - model_name: llama-3.3-70b-versatile\n",
    "    litellm_params:\n",
    "      model: groq/llama-3.3-70b-versatile\n",
    "      api_key: os.environ/GROQ_API_KEY_153\n",
    "      api_base: https://api.groq.com/openai/v1\n",
    "\n",
    "litellm_settings:\n",
    "  num_retries: 3\n",
    "  request_timeout: 6000\n",
    "  allowed_fails: 1\n",
    "  cooldown_time: 30\n",
    "  drop_params: true\n",
    "  \n",
    "router_settings:\n",
    "  routing_strategy: \"usage-based-routing-v2\"\n",
    "  \n",
    "general_settings:\n",
    "  port: 4000\n",
    "  cache: false \n",
    "\"\"\"\n",
    "\n",
    "    # Add Ollama models only for LOCAL environment\n",
    "    if environment == LOCAL:\n",
    "        ollama_models = \"\"\"\n",
    "  # Ollama local models\n",
    "  - model_name: \"qwen3-8b-GGUF-q8_0_local\"             \n",
    "    litellm_params:\n",
    "      model: \"ollama/qwen3-8b-GGUF-q8_0\"\n",
    "      api_base: \"http://localhost:11434\"\n",
    "      api_key: ollama\n",
    "  - model_name: \"DeepSeek-R1-0528-Qwen3-8B-GGUF-Q8_K_XL_local\"             \n",
    "    litellm_params:\n",
    "      model: \"ollama/DeepSeek-R1-0528-Qwen3-8B-GGUF-Q8_K_XL\"\n",
    "      api_base: \"http://localhost:11434\"\n",
    "      api_key: ollama\n",
    "      num_ctx: 130000\n",
    "  - model_name: \"qwen3-8B-192k-GGUF-IQ4_XS_local\"             \n",
    "    litellm_params:\n",
    "      model: \"ollama/qwen3-8B-192k-Context-6X-Larger-GGUF-IQ4_XS\"\n",
    "      api_base: \"http://localhost:11434\"\n",
    "      api_key: ollama\n",
    "\"\"\"\n",
    "        # Insert Ollama models at the beginning\n",
    "        lines = config_content.split('\\n')\n",
    "        # Find the line with \"model_list:\" and insert after it\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip() == \"model_list:\":\n",
    "                lines.insert(i + 1, ollama_models)\n",
    "                break\n",
    "        config_content = '\\n'.join(lines)\n",
    "        logging.info(\"Added Ollama models for LOCAL environment\")\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    logging.info(f\"Config file created at: {config_path}\")\n",
    "    return config_path\n",
    "\n",
    "def start_litellm_proxy(config_path, port=4000):\n",
    "    \"\"\"\n",
    "    Starts LiteLLM proxy server in background.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to config file\n",
    "        port: Port to run proxy on\n",
    "    \n",
    "    Returns:\n",
    "        subprocess.Popen: Process handle\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"litellm\",\n",
    "        \"--config\", config_path,\n",
    "        \"--port\", str(port),\n",
    "        \"--detailed_debug\"\n",
    "    ]\n",
    "    \n",
    "    logging.info(f\"Starting LiteLLM proxy on port {port}...\")\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    # Wait a bit for server to start\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Check if process is running\n",
    "    if process.poll() is None:\n",
    "        logging.info(f\"âœ… LiteLLM proxy started successfully on http://0.0.0.0:{port}\")\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    "        logging.error(f\"Failed to start proxy:\\n{stderr}\")\n",
    "        raise RuntimeError(\"LiteLLM proxy failed to start\")\n",
    "    \n",
    "    return process\n",
    "\n",
    "def main():\n",
    "    (env_name, working_dir, python_version, wandb_api_key,) = get_environment()\n",
    "\n",
    "    if wandb_api_key:\n",
    "        os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "        wandb_api_key_length = len(wandb_api_key)\n",
    "    else:\n",
    "        wandb_api_key_length = 0\n",
    "\n",
    "    os.environ[\"ENVIRONMENT\"] = env_name\n",
    "    os.environ[\"WORKING_DIR\"] = working_dir\n",
    "    os.environ[\"WANDB_ENTITY\"] = \"rogermt23\"\n",
    "    os.environ[\"WANDB_PROJECT_NAME\"] = \"nsgf-paper2code-playground\"\n",
    "    os.environ[\"WEAVE_PROJECT_NAME\"] = \"nsgf-paper2code-playground\"\n",
    "\n",
    "    os.environ[\"OPENAI_BASE_URL\"] = \"http://0.0.0.0:4000\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"litellm\"\n",
    "    \n",
    "    # Display environment information\n",
    "    print(f\"Environment          : {env_name}\")\n",
    "    print(f\"Working Directory    : {working_dir}\")\n",
    "    print(f\"Python Version       : {python_version}\")\n",
    "    print(f\"Wandb API Key Length : {wandb_api_key_length}\")\n",
    "\n",
    "    setup_and_start_ollama([\n",
    "        #\"hf.co/Qwen/Qwen3-8B-GGUF:Q8_0\",\n",
    "        \"hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL\"\n",
    "    ])\n",
    "\n",
    "    # Create config and start proxy\n",
    "    config_path = os.path.join(working_dir, \"litellm_config.yaml\")\n",
    "    create_litellm_config(config_path, env_name)\n",
    "    \n",
    "    proxy_process = start_litellm_proxy(config_path)\n",
    "    \n",
    "    return working_dir, python_version, proxy_process\n",
    "\n",
    "# Run the setup in the notebook\n",
    "working_dir, python_version, proxy_process = main()\n",
    "\n",
    "print(\"\\nâœ… Setup complete! LiteLLM proxy is running.\")\n",
    "print(\"Use OpenAI SDK with base_url='http://0.0.0.0:4000' and api_key='litellm'\")\n",
    "\n",
    "# Show available models based on environment\n",
    "if os.environ[\"ENVIRONMENT\"] == LOCAL:\n",
    "    print(\"\\nðŸ“¦ Available models (LOCAL):\")\n",
    "    print(\"  Local Ollama:\")\n",
    "    print(\"    - qwen3-8b-GGUF-q8_0_local\")\n",
    "    print(\"    - DeepSeek-R1-0528-Qwen3-8B-GGUF-Q8_K_XL_local\")\n",
    "    print(\"    - qwen3-8B-192k-GGUF-IQ4_XS_local\")\n",
    "    print(\"  Cloud (with account rotation):\")\n",
    "    print(\"    - deepseek-chat-v3 (OpenRouter x2)\")\n",
    "    print(\"    - o3-mini, o3-mini_153 (Cerebras)\")\n",
    "    print(\"    - qwen-3-235b-a22b-thinking-2507 (x2)\")\n",
    "    print(\"    - qwen-3-235b-a22b-instruct-2507 (x2)\")\n",
    "    print(\"    - gpt-oss-120b (Groq x2)\")\n",
    "    print(\"    - llama-3.3-70b-versatile (Groq x2)\")\n",
    "else:\n",
    "    print(\"\\nðŸ“¦ Available models (CLOUD):\")\n",
    "    print(\"    - deepseek-chat-v3 (OpenRouter x2)\")\n",
    "    print(\"    - o3-mini, o3-mini_153 (Cerebras)\")\n",
    "    print(\"    - qwen-3-235b-a22b-thinking-2507 (x2)\")\n",
    "    print(\"    - qwen-3-235b-a22b-instruct-2507 (x2)\")\n",
    "    print(\"    - gpt-oss-120b (Groq x2)\")\n",
    "    print(\"    - llama-3.3-70b-versatile (Groq x2)\")\n",
    "\n",
    "print(\"\\nExample:\")\n",
    "print(\"\"\"\n",
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:4000\", api_key=\"litellm\")\n",
    "\n",
    "# Auto-rotates through both accounts!\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss-120b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $working_dir\n",
    "# Download NSGF paper PDF using curl (or use wget: replace with !wget -O ...)\n",
    "!curl -o /content/NSGF.pdf https://arxiv.org/pdf/2401.14069.pdf\n",
    "\n",
    "\n",
    "%cd $working_dir\n",
    "!git clone -b feature/litellm-support https://github.com/rogermt/Paper2Code.git\n",
    "!git ls-remote https://github.com/rogermt/Paper2Code.git refs/heads/feature/litellm-support\n",
    "\n",
    "%cd Paper2Code\n",
    "!ls -l $working_dir/Paper2Code/codes\n",
    "%pip install -r requirements.txt -q\n",
    "%pip install pydantic==2.11.9 --upgrade\n",
    "\n",
    "%cd $working_dir/Paper2Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ask GitHub what SHA it has for the branch (no clone needed)\n",
    "!echo \"remote SHA advertised for feature/litellm-support:\"\n",
    "!git ls-remote https://github.com/rogermt/Paper2Code.git refs/heads/feature/litellm-support\n",
    "\n",
    "# 2) (Optional) show the SHA you expect (replace if different)\n",
    "!echo \"expected SHA (from your laptop): 1c9e4ac\"\n",
    "\n",
    "# 3) Clone fresh into a new folder to avoid any cache\n",
    "!rm -rf /kaggle/working/Paper2Code_clone_test\n",
    "!git clone -b feature/litellm-support https://github.com/rogermt/Paper2Code.git /kaggle/working/Paper2Code_clone_test\n",
    "\n",
    "# 4) Show what commit the new clone checked out\n",
    "!echo \"clone HEAD (short):\"\n",
    "!/bin/bash -c \"cd /kaggle/working/Paper2Code_clone_test && git rev-parse --short HEAD\"\n",
    "\n",
    "# 5) Show the first-level file list and codes folder listing so we can confirm which files are present\n",
    "!echo \"top-level files in clone:\"\n",
    "!/bin/bash -c \"ls -la /kaggle/working/Paper2Code_clone_test | sed -n '1,200p'\"\n",
    "!echo \"codes/ listing:\"\n",
    "!/bin/bash -c \"ls -la /kaggle/working/Paper2Code_clone_test/codes | sed -n '1,200p'\"\n",
    "\n",
    "# 6) Also show the remote commits (last 5) on the remote branch (server-side view)\n",
    "!echo \"recent remote commits (server-side) for origin branch:\"\n",
    "!/bin/bash -c \"git ls-remote --refs https://github.com/rogermt/Paper2Code.git | grep feature/litellm-support -n || true\"\n",
    "!/bin/bash -c \"git ls-remote https://github.com/rogermt/Paper2Code.git refs/heads/feature/litellm-support || true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama cp hf.co/Qwen/Qwen3-8B-GGUF:Q8_0 qwen3-8b-GGUF-q8_0\n",
    "!ollama cp hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL DeepSeek-R1-0528-Qwen3-8B-GGUF-Q8_K_XL\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anKXA59AJZqc",
    "outputId": "ab88912e-dec4-48f1-800e-bb5b6df7681d"
   },
   "outputs": [],
   "source": [
    "%%writefile $working_dir/Paper2Code/notebooks/test_ollama_openai_proxy.py\n",
    "import argparse\n",
    "from openai import OpenAI\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Test Ollama OpenAI proxy with a chat completion.\")\n",
    "    parser.add_argument(\n",
    "        \"--base_url\",\n",
    "        type=str,\n",
    "        default=\"http://localhost:11434/v1\",\n",
    "        help=\"Base URL of the OpenAI-compatible proxy (default: http://localhost:11434/v1)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        default=\"hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL\",\n",
    "        help=\"Model name to use (default: DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=args.base_url,\n",
    "        api_key=\"ollama\"  # Dummy key for local proxy\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=args.model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Capital city of France.\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $working_dir/Paper2Code/notebooks/test_ollama_openai_proxy_dspy.py\n",
    "import argparse\n",
    "import dspy\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Test DSPy with Ollama OpenAI proxy.\")\n",
    "    parser.add_argument(\n",
    "        \"--base_url\",\n",
    "        type=str,\n",
    "        default=\"http://localhost:11434/v1\",\n",
    "        help=\"Base URL of the OpenAI-compatible proxy (default: http://localhost:11434/v1)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        default=\"ollama/DeepSeek-R1-0528-Qwen3-8B-GGUF-Q8_K_XL:latest\",\n",
    "        help=\"Model name to use (default: DeepSeek-R1-0528-Qwen3-8B-GGUF-Q8_K_XL:latest)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Your original code, untouched except for args\n",
    "    lm = dspy.LM(\n",
    "        model=args.model,\n",
    "        # api_base=args.base_url,  # Left commented as in your original\n",
    "        api_key=\"ollama\",\n",
    "        max_tokens=256,\n",
    "        temperature=0.2,\n",
    "        cache=False,\n",
    "    )\n",
    "\n",
    "    dspy.settings.configure(lm=lm)\n",
    "\n",
    "    class Summarize(dspy.Signature):\n",
    "        \"\"\"Summarize a passage into one sentence.\"\"\"\n",
    "        context: str = dspy.InputField()\n",
    "        summary: str = dspy.OutputField()\n",
    "\n",
    "    class Summarizer(dspy.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.predict = dspy.Predict(Summarize)\n",
    "\n",
    "        def forward(self, context: str):\n",
    "            return self.predict(context=context)\n",
    "\n",
    "    passage = \"Ollama exposes an OpenAI-compatible API on port 11434, so DSPy can connect directly without a proxy.\"\n",
    "    module = Summarizer()\n",
    "    pred = module(context=passage)\n",
    "    print(\"Summary:\", pred.summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $working_dir/Paper2Code/notebooks/test_litellm_openai_proxy_dspy.py\n",
    "import dspy\n",
    "from typing import List\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "# Define get_prefixed_model\n",
    "def get_prefixed_model(model_name: str) -> str:\n",
    "    \"\"\"Map model alias to prefixed model for compatibility.\"\"\"\n",
    "    if model_name == \"qwen3-8b-GGUF-q8_0_local\":\n",
    "        return \"ollama/qwen3-8b-GGUF-q8_0\"\n",
    "    return f\"openai/{model_name}\"\n",
    "\n",
    "# Configure DSPy with direct Ollama endpoint\n",
    "gpt_version = \"DeepSeek-R1-0528-Qwen3-8B-GGUF-Q8_K_XL_local\"\n",
    "prefixed_model = get_prefixed_model(gpt_version)\n",
    "lm = dspy.LM(\n",
    "    model=prefixed_model,\n",
    "    api_base=\"http://localhost:4000\",   # LiteLLM proxy\n",
    "    api_key=\"anything\",                 # proxy accepts any key if master_key not set\n",
    "    cache=False\n",
    ")\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "# Define CodeEvaluation signature\n",
    "class CodeEvaluation(dspy.Signature):\n",
    "    \"\"\"Evaluate code based on paper description. Return a JSON object with 'score' (1-5, based on correctness, completeness, and adherence to the paper's methodology) and 'critique_list' (exactly 3 concise critiques, each <=50 characters, explaining the score). An optional explanation may be included for additional context.\"\"\"\n",
    "    paper = dspy.InputField(desc=\"Description of the paper\")\n",
    "    code = dspy.InputField(desc=\"Code implementation to evaluate\")\n",
    "    score = dspy.OutputField(desc=\"Score (1-5)\", prefix=\"Score (1-5):\")\n",
    "    critique_list = dspy.OutputField(\n",
    "        desc=\"List of 3 critiques, each <=50 characters\",\n",
    "        prefix=\"Critique List:\"\n",
    "    )\n",
    "\n",
    "# Define decorated function to trace DSPy Predict\n",
    "#@weave.op()\n",
    "def evaluate_code(paper: str, code: str):\n",
    "    evaluator = dspy.Predict(CodeEvaluation)\n",
    "    response = evaluator(paper=paper, code=code)\n",
    "    return response\n",
    "\n",
    "# Create a large input (~100,000 tokens)\n",
    "large_paper = \"Abstract: This paper introduces a novel algorithm for graph neural networks. \" * 50 # ~70,000 tokens\n",
    "large_code = \"\"\"\n",
    "def graph_neural_network(data):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    class GNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(GNN, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "\"\"\" * 10  # ~30,000 tokens\n",
    "\n",
    "# Count tokens\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "paper_tokens = len(encoding.encode(large_paper, allowed_special={\"<|endoftext|>\"}))\n",
    "code_tokens = len(encoding.encode(large_code, allowed_special={\"<|endoftext|>\"}))\n",
    "print(f\"Paper tokens: {paper_tokens}, Code tokens: {code_tokens}, Total: {paper_tokens + code_tokens}\")\n",
    "\n",
    "# Try to evaluate without chunking\n",
    "try:\n",
    "    response = evaluate_code(large_paper, large_code)  # Call decorated function\n",
    "    print(\"Score:\", response.score)\n",
    "    print(\"Critique:\", response.critique_list)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-inNxItg8fq5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python ${WORKING_DIR}/Paper2Code/notebooks/test_ollama_openai_proxy.py\n",
    "python ${WORKING_DIR}/Paper2Code/notebooks/test_ollama_openai_proxy_dspy.py\n",
    "#python ${WORKING_DIR}/Paper2Code/notebooks/test_litellm_openai_proxy_dspy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $working_dir/Paper2Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aqai_Jklqk55",
    "outputId": "597c5069-34f3-4c12-976c-5d03f649a23a"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir $WORKING_DIR/output\n",
    "cd $WORKING_DIR/output; wget --spider https://raw.githubusercontent.com/rogermt/playground/master/src/NSGF.json \\\n",
    "  && wget https://raw.githubusercontent.com/rogermt/playground/master/src/NSGF.json\n",
    "cat ${WORKING_DIR}/output/NSGF.json  | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "use_kaggle_resumer = True\n",
    "import shutil\n",
    "\n",
    "use_kaggle_resumer = False\n",
    "if use_kaggle_resumer:\n",
    "    working_dir = \"/kaggle/working\"\n",
    "    kaggle_input_dir = \"/kaggle/input\"\n",
    "    output_dir  = f\"{working_dir}/output/NSGF\"\n",
    "    kaggle_resume_dir=\"paper2code-nsgf-paper-output\"\n",
    "    resume_dir = f\"{kaggle_input_dir}/{kaggle_resume_dir}/output/NSGF\"\n",
    "    plan_traj_json = f'{output_dir}/planning_trajectories.json'\n",
    "    plan_traj_full_json = f\"{working_dir}/planning_trajectories_full.json\"\n",
    "    print(f\"{resume_dir}=>{output_dir}\")\n",
    "    shutil.copytree(resume_dir, output_dir, dirs_exist_ok=True)\n",
    "    shutil.copy(plan_traj_full_json, plan_traj_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $working_dir/Paper2Code/scripts\n",
    "!bash run_litellm.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $working_dir/Paper2Code/codes\n",
    "!python eval_litellm.py \\\n",
    "  --paper_name NSGF \\\n",
    "  --gpt_version \"DeepSeek-R1-0528-Qwen3-8B-GGUF-Q8_K_XL_local\" \\\n",
    "  --pdf_json_path $working_dir/output/NSGF.json \\\n",
    "  --data_dir $working_dir/Paper2Code/data \\\n",
    "  --output_dir $working_dir/output/NSGF \\\n",
    "  --target_repo_dir $working_dir/output/NSGF_repo \\\n",
    "  --eval_result_dir $working_dir/results \\\n",
    "  --eval_type ref_free \\\n",
    "  --generated_n 1 \\\n",
    "  --papercoder \\\n",
    "  --max_context 120000 \\\n",
    "  --delay 0 \\\n",
    "  --temperature 0.6 \\\n",
    "  --wandb_run_id rwo1law5"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8309657,
     "sourceId": 13117590,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
